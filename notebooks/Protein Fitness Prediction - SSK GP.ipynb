{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Regression on Protein Sequences: Subsequence String Kernel #\n",
    "\n",
    "An example notebook for string kernel-based GP regression on a dataset of protein sequences using the subsequence string kernel (SSK) model of [1, 2]. For the bag-of-amino acids representation of the protein sequence (analagous to the bag-of-SMILES model for molecules) see the 'protein fitness prediction - bag of amino acids notebook'. The protein dataset consists of 151 sequences with a 'fitness' function (target label) of the melting point in degrees Celcius. The dataset is collated from values reported in references [3,4,5]. The sequences are each of length 290 and so it is recommended that a GPU is used in conjunction with the SSK kernel.\n",
    "\n",
    "In contrast to the bag of amino acids notebook, we do not report results on 20 random train/test splits because this would be too computationally intensive for the SSK kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports\"\"\"\n",
    "\n",
    "# Turn off Graphein warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# To import from the gauche package\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from botorch import fit_gpytorch_model\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.transforms import Normalize, Standardize\n",
    "from botorch.models.fully_bayesian import MIN_INFERRED_NOISE_LEVEL\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.priors import GammaPrior\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import torch\n",
    "\n",
    "from gauche.dataloader.data_utils import transform_data\n",
    "from gauche.kernels.string_kernels.sskkernel import pad, encode_string, build_one_hot, SubsequenceStringKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dtype': torch.float32, 'device': device(type='cpu')}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"CPU/GPU\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tkwargs = {\"dtype\": torch.float, \"device\": device}\n",
    "print(tkwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Petase Dataset\n",
    "\n",
    "The dataset consists of a set of petase protein sequences with amino acid chains of length 290. An example sequence is given below:\n",
    "\n",
    "```\n",
    "MNFPRASRLMQAAVLGGLMAVSAAATAQTNPYARGPPPTAASLEASAGPFTVRSFTVSRPSGYGAGTVYYPTNAGGTVGAIAIVPGYTARQSSIKWWGPRLASHGFVVITIDTNSTLDQPSSRSSQQMAALRQVASLNGTSSSPIYGKVDTARMGVMGWSMGGGGSLISAANNPSLKAAAPQAPWDSSTNFSSVTVPTLIFACENDSIAPVNSSALPIYDSMSRNAKQFLEINGGSHSCANSGNSNQALIGKKGVAWMKRFMDNDTRYSTFACENPNSTRVSDFRTANCS\n",
    "```\n",
    "\n",
    "For such long sequences the SSK kernel can struggle computationally and so a \"bag of amino acids\" model is also compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Regression experiments parameters, number of random splits and split size\"\"\"\n",
    "\n",
    "n_trials = 20\n",
    "test_set_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sequences) 151 | len(targets) 151\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load the petase dataset\"\"\"\n",
    "\n",
    "df = pd.read_csv('../data/proteins/petase_151_mutants.csv')\n",
    "x = df['sequence'].to_list()\n",
    "y = df['fitness'].to_numpy().reshape(-1, 1)\n",
    "print(f'len(sequences) {len(x)} | len(targets) {len(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphabet \n",
      " ['L', 'I', 'A', 'F', 'Q', 'Y', 'S', 'W', 'E', 'H', 'D', 'R', 'P', 'N', 'T', 'K', 'V', 'M', 'G', 'C'] \n",
      " length of alphabet 20\n",
      "maxlen 290\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Compute the required sequence properties for modelling with the SSK kernel GP.\"\"\"\n",
    "\n",
    "maxlen = np.max([len(seq) for seq in x])\n",
    "# get alphabet of characters used in candidate set (to init SSK)\n",
    "alphabet = list({l for word in x for l in word})\n",
    "print(f'alphabet \\n {alphabet} \\n length of alphabet {len(alphabet)}')\n",
    "print(f'maxlen {maxlen}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Regression on the Petase Dataset\n",
    "\n",
    "First we define the GP model for protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Process the inputs x to the string kernel GPs\"\"\"\n",
    "\n",
    "# Compute one-hot encodings and an integer index for the given amino acid alphabet\n",
    "embds, index = build_one_hot(alphabet)\n",
    "embds = embds.to(**tkwargs)\n",
    "\n",
    "# Process the string inputs to the SSK model\n",
    "x = torch.cat([pad(encode_string(seq, index), maxlen).unsqueeze(0) for seq in x], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute the train/test split.\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_set_size, random_state=0)\n",
    "X_train = X_train.to(**tkwargs)\n",
    "X_test = X_test.to(**tkwargs)\n",
    "y_train = torch.tensor(y_train, **tkwargs)\n",
    "y_test = torch.tensor(y_test, **tkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Intialize and fit the models\"\"\"\n",
    "\n",
    "# Likelihood function\n",
    "likelihood = GaussianLikelihood(\n",
    "    noise_prior=GammaPrior(torch.tensor(0.9, **tkwargs), torch.tensor(10.0, **tkwargs)),\n",
    "    noise_constraint=GreaterThan(MIN_INFERRED_NOISE_LEVEL),\n",
    ")\n",
    "\n",
    "# Covariance function\n",
    "covar_module = ScaleKernel(SubsequenceStringKernel(embds, index, alphabet, maxlen, **tkwargs))\n",
    "\n",
    "\n",
    "ssk_gp_model = SingleTaskGP(\n",
    "    train_X=X_train,\n",
    "    train_Y=y_train,\n",
    "    outcome_transform=Standardize(1),\n",
    "    likelihood=likelihood,\n",
    "    covar_module=covar_module,\n",
    ")\n",
    "\n",
    "mll = ExactMarginalLogLikelihood(model=ssk_gp_model, likelihood=ssk_gp_model.likelihood)\n",
    "# ideally we can optimize over the kernel hyper-parameters of the string kernel\n",
    "# however, the gpu memory usage in batch (GPU) version of the kernel is quite high\n",
    "# while the standard non-batch version is relatively slow for kernel evaluation.\n",
    "# Nevertheless, the kernel is very robust to choices of the different hypers.\n",
    "mll.model.covar_module.base_kernel.raw_order_coefs.requires_grad = False\n",
    "mll.model.covar_module.base_kernel.raw_match_decay.requires_grad = False\n",
    "mll.model.covar_module.base_kernel.raw_gap_decay.requires_grad = False\n",
    "\n",
    "fit_gpytorch_model(mll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate the trained model.\"\"\"\n",
    "\n",
    "posterior = ssk_gp_model.posterior(X_test)\n",
    "posterior_mean = posterior.mean.cpu().detach()\n",
    "posterior_std = torch.sqrt(posterior.variance.cpu().detach())\n",
    "\n",
    "r2 = r2_score(y_test, posterior_mean.numpy())\n",
    "print(mean_absolute_error(posterior_mean.squeeze(1), y_test.cpu().detach().squeeze(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot the R^2\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (16, 6))\n",
    "ax = ax.reshape(-1)\n",
    "\n",
    "ax.scatter(y_test, posterior_mean.numpy())\n",
    "ax.set_title(f'Test set $R^2 = {r2:.2f}$')\n",
    "ax.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, posterior_mean.numpy(), 1)(np.unique(y_test)), color='k', linewidth=0.4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N. and Watkins, C., 2002. [Text classification using string kernels](https://jmlr.csail.mit.edu/papers/volume2/lodhi02a/lodhi02a.pdf). The Journal of Machine Learning Research, pp.419-444.\n",
    "\n",
    "[2] Cancedda, N., Gaussier, E., Goutte, C. and Renders, J.M., 2003. [Word sequence kernels.](https://www.jmlr.org/papers/volume3/cancedda03a/cancedda03a.pdf) The Journal of Machine Learning Research, pp.1059-1082.\n",
    "\n",
    "[3] Cui, Y., Chen, Y., Liu, X., Dong, S., Tian, Y.E., Qiao, Y., Mitra, R., Han, J., Li, C., Han, X. and Liu, W., 2021. [Computational redesign of a PETase for plastic biodegradation under ambient condition by the GRAPE strategy](https://pubs.acs.org/doi/abs/10.1021/acscatal.0c05126). ACS Catalysis, 11(3), pp.1340-1350.\n",
    "\n",
    "[4] Liu, B., He, L., Wang, L., Li, T., Li, C., Liu, H., Luo, Y. and Bao, R., 2018. [Protein crystallography and site‚Äêdirect mutagenesis analysis of the poly (ethylene terephthalate) hydrolase PETase from Ideonella sakaiensis](https://chemistry-europe.onlinelibrary.wiley.com/doi/abs/10.1002/cbic.201800097). ChemBioChem, 19(14), pp.1471-1475.\n",
    "\n",
    "[5] Joo, S., Cho, I.J., Seo, H., Son, H.F., Sagong, H.Y., Shin, T.J., Choi, S.Y., Lee, S.Y. and Kim, K.J., 2018. [Structural insight into molecular mechanism of poly (ethylene terephthalate) degradation](https://www.nature.com/articles/s41467-018-02881-1). Nature communications, 9(1), p.382."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d0027c8e4af6ef19652b5e9178dbab0fb836d7604e2f6f0710420cbb45dd4f7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
